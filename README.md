## Enhancing SAR Image Interpretability through Colorization using Deep Learning
An innovative solution to colorize grayscale SAR images by creating a deep learning model that efficiently and accurately predicts colors from pairs of SAR and optical images, enhancing the usability of SAR data in fields like geological studies and environmental monitoring.


## About
Synthetic Aperture Radar (SAR) is a form of radar used to create images of objects, such as landscapes or buildings. Unlike optical imaging, SAR can capture data in all weather conditions, at any time of day, and through various atmospheric conditions. SAR images are typically grayscale and provide information about the surface's backscatter properties rather than its color.For SAR image colorization, deep learning models can learn to map grayscale SAR data to color images by learning from large datasets of labeled examples.Colorization can assist in better interpretation for applications like land use classification, change detection, and target identification.


## Features
- Vegetation and Water Body Differentiation
- Enhanced Visualization
- Geometric Corrections
- Texture Analysis
- Temporal Analysis

## Requirements
* Operating System: Requires a 64-bit OS (Windows 10 or Ubuntu) for compatibility with deep learning frameworks.
* Development Environment: Python 3.6 or later is necessary for coding the sign language detection system.
* Deep Learning Frameworks: TensorFlow for model training, MediaPipe for hand gesture recognition.
* Image Processing Libraries: OpenCV is essential for efficient image processing and real-time hand gesture recognition.
* Version Control: Implementation of Git for collaborative development and effective code management.
* IDE: Use of VSCode as the Integrated Development Environment for coding, debugging, and version control integration.
* Additional Dependencies: Includes scikit-learn, TensorFlow (versions 2.4.1), TensorFlow GPU, OpenCV, and Mediapipe for deep learning tasks.

## System Architecture
![alt text](img/Architecture.png)

## Source Code
Login Form.html
```
<!doctype html>
<html lang="en">
<head>
    <meta charset="UTF-8">  
    <title>Login Form in HTML & CSS</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@100..900&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="styles.css"> 
</head>
<body>
    <div class="form-area">
        <div class="wrapper">
            <h2>Login</h2>       
            <form>             
                <div class="box">
                    <input type="text" placeholder="Username">
                    <i class="fa fa-user"></i>
                </div>
                <div class="box">
                    <input type="password" placeholder="Password">
                    <i class="fa fa-lock"></i>
                </div>
                <div class="options">
                    <label><input type="checkbox"> Remember Me</label>
                    <a href="#">Forgot Password?</a>
                </div>
                <button type="submit">Submit</button>
            </form>
          </div>
    </div>
   </body>
</html>
```
Styles.css
```
* {
	box-sizing: border-box; 
}
body {
	margin: 0;
	padding: 0;
	font-family: "Montserrat", sans-serif;
	height: 100vh;
	width: 100%;
	background: linear-gradient(to bottom, black, transparent), url("1.jpg");
	-webkit-background-size: cover;
	background-size: cover;
	background-repeat: no-repeat;
	background-position: center center;
}
.wrapper {
	width: 400px;
	height: auto;
	color: #fff;
	padding: 50px 30px;
	box-shadow: 0 0 1rem 0 rgba(0, 0, 0, .2);
	position: relative;
	background: #ffffff1a;
	border: 2px solid #ffffff30;
	box-shadow: 0 0 30px #0000002a;
	border-radius: 25px;
}
.wrapper h2 {
	text-align: center;
	margin: 0;
	margin-bottom: 30px;
}
.wrapper p {
	margin: 0;
	padding: 0;
	font-weight: 300;
	text-align: center;
}
.form-area {
	display: grid;
	place-items: center;
	height: 100vh;
}
.box {
	position: relative;
}
.box input {
	padding-right: 30px;
}
.box i {
	position: absolute;
	top: 34%;
	transform: translateY(-50%);
	right: 30px;
	color: #fff;
}
.wrapper input, button {
	border: none;
	border: 1px solid #bababa;
	background: transparent;
	outline: none;
	height: 50px;
	color: #ffffff;
	font-size: 16px;
	width: 100%;
	margin-bottom: 20px;
	padding: 15px;
	border-radius: 50px;
}
.wrapper .options {
	display: flex;
	justify-content: space-between;
	align-items: center;
	margin-bottom: 20px;
}
.wrapper .options label {
	color: #fff;
	font-size: 18px;
	line-height: 3.5;
}
.options input[type="checkbox"] {
	width: 15px;
	height: auto;
}
.wrapper a {
	text-decoration: none;
	color: #fff;
}
.wrapper button {
	background: #fff;
	color: #262626;
}
```
.ipynb
```
from google.colab import drive
import zipfile
import os

drive.mount('/content/drive')

zip_path = '/content/drive/MyDrive/SAR_2/v_2.zip'  # Update with actual path
extract_path = '/content/sar_dataset'
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)
```
```
import torch
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
import random
import glob

class SARColorizationDataset(Dataset):
    def __init__(self, base_dir, transform=None, limit=5):
        self.base_dir = base_dir
        self.transform = transform
        self.classes = ['agri', 'barrenland', 'grassland', 'urban']
        self.image_pairs = self._get_image_pairs(limit)

    def _get_image_pairs(self, limit):
        image_pairs = []
        for category in self.classes:
            s1_images = sorted(glob.glob(os.path.join(self.base_dir, category, 's1', '*.png')))
            s2_images = sorted(glob.glob(os.path.join(self.base_dir, category, 's2', '*.png')))
            pairs = list(zip(s1_images, s2_images))
            image_pairs.extend(pairs[:limit])
        return image_pairs

    def __len__(self):
        return len(self.image_pairs)

    def __getitem__(self, idx):
        s1_image_path, s2_image_path = self.image_pairs[idx]
        s1_image = Image.open(s1_image_path).convert("L")
        s2_image = Image.open(s2_image_path).convert("RGB")

        if self.transform:
            s1_image = self.transform(s1_image)
            s2_image = self.transform(s2_image)

        return s1_image, s2_image

# Define transformations
transform = transforms.Compose([
    transforms.Resize((128, 128)),
    transforms.ToTensor()
])

# Instantiate dataset and dataloader
dataset_path = "/content/sar_dataset"
dataset = SARColorizationDataset(dataset_path, transform=transform, limit=5)
dataloader = DataLoader(dataset, batch_size=1, shuffle=True)
```
```
import torch.nn as nn
import torch.optim as optim

# Colorization Network (simplified encoder-decoder)
class SARColorizationNet(nn.Module):
    def __init__(self):
        super(SARColorizationNet, self).__init__()
        self.encoder = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3, stride=2, padding=1),
            nn.ReLU(),
            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),
            nn.ReLU(),
            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),
            nn.ReLU()
        )
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),
            nn.ReLU(),
            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),
            nn.ReLU(),
            nn.ConvTranspose2d(64, 3, kernel_size=3, stride=2, padding=1, output_padding=1),
            nn.Sigmoid()
        )

    def forward(self, x):
        x = self.encoder(x)
        x = self.decoder(x)
        return x

# Initialize model, loss, and optimizer
model = SARColorizationNet()
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

for epoch in range(num_epochs):
    for i, (grayscale, color) in enumerate(dataloader):
        # Remove the extra unsqueeze operation:
        # grayscale = grayscale.unsqueeze(1)  # Remove this line
        # The input should already be in the correct format [B, C, H, W]
        # which is [1, 1, 128, 128] in your case.

        color = color

        outputs = model(grayscale)
        loss = criterion(outputs, color)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        print(f"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(dataloader)}], Loss: {loss.item():.4f}")

print("Training complete.")
```
```
import matplotlib.pyplot as plt

def visualize_results(model, dataloader):
    model.eval()
    with torch.no_grad():
        for i, (grayscale, color) in enumerate(dataloader):
            if i == 5:  # Limit to 5 visualizations
                break
            # Remove the extra unsqueeze operation:
            # grayscale = grayscale.unsqueeze(1)
            # The input should already be in the correct format [B, C, H, W]
            # which is [1, 1, 128, 128] in your case.
            colorized_output = model(grayscale).squeeze().permute(1, 2, 0).numpy()

            # Plot
            plt.figure(figsize=(12, 4))
            plt.subplot(1, 3, 1)
            plt.imshow(grayscale.squeeze(), cmap='gray')
            plt.title("Grayscale Input")

            plt.subplot(1, 3, 2)
            plt.imshow(color.squeeze().permute(1, 2, 0).numpy())
            plt.title("Original Color Image")

            plt.subplot(1, 3, 3)
            plt.imshow(colorized_output)
            plt.title("Colorized Output")
            plt.show()

# Visualize colorization results
visualize_results(model, dataloader)
```
## Output

#### Output1 - Login Page
![alt text](<img/Screenshot 2024-10-26 001155.png>)
![1OUTPUT SAR](https://github.com/user-attachments/assets/61196477-425d-41a8-973c-10e25f1d7ccc)


#### Output2 - Train the Model
![2OUTPUT](https://github.com/user-attachments/assets/4ff62d57-25df-41c3-be7c-b4649324a099)

#### Output2 - SAR Image Colorization
![3OUTPUT](https://github.com/user-attachments/assets/8e2becf4-7d94-4f8b-ac03-d415cc7753ee)

Calculated MSE: 0.034

## Results and Impact

The proposed system enhances the interpretability of SAR images across different applications, enabling improved performance in tasks such as object detection, change monitoring, and feature extraction. The model achieves remarkable accuracy, with an average Structural Similarity Index (SSIM) of 0.93, Mean Squared Error (MSE) of 0.02, and Root Mean Squared Error (RMSE) of 0.14, underscoring its effectiveness in colorizing SAR images while preserving important details.

In addition to its accuracy, our system demonstrates high efficiency, with an average processing time of 0.05 seconds per image on an NVIDIA GeForce RTX 2080 Ti GPU. This efficient performance makes our system practical for real-world applications, where rapid analysis of SAR imagery is essential. By enhancing SAR images through colorization, our system contributes to more insightful and precise decision-making across fields that rely on SAR data.

## References
1. Xiaochun Mai, Meilu Zhu, Yixuan Yuan, "CMCNet: Colorization-Aware Mix-Uncertainty-Adaptive Consistency Network for Semi-Supervised Fruit Counting", IEEE Transactions on Automation Science and Engineering, vol. 21, no. 4, 2024.
2. Jaehyup Lee, Hyebin Cho, Doochun Seo, Hyun-Ho Kim, Jaeheon Jeong, Munchurl Kim, "CFCA-SET: Coarse-to-Fine Context-Aware SAR-to-EO Translation With Auxiliary Learning of SAR-to-NIR Translation", IEEE Transactions on Geoscience and Remote Sensing, vol. 61, 2023.
3. Qian Song, Feng Xu, Ya-Qiu Jin, "Radar Image Colorization: Converting Single-Polarization to Fully Polarimetric Using Deep Neural Networks", IEEE Access, vol. 6, 2018.
